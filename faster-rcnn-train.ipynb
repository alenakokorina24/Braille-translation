{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport sys\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport PIL\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data_utils\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.rpn import AnchorGenerator","metadata":{"_uuid":"2ab640ac-eb01-4185-b00e-da5672135339","_cell_guid":"cd837b5c-ebd3-4949-9a6a-6c095f974d95","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-05-24T13:31:49.601081Z","iopub.execute_input":"2021-05-24T13:31:49.601403Z","iopub.status.idle":"2021-05-24T13:31:49.609714Z","shell.execute_reply.started":"2021-05-24T13:31:49.601366Z","shell.execute_reply":"2021-05-24T13:31:49.608716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BrailleDataset(torch.utils.data.Dataset):\n  def __init__(self, images, labels):\n    \"\"\"\n    Args:\n        images (string): Directory with all the images.\n        labels (string): Path to a csv file with labels.\n    \"\"\"\n    df = pd.read_csv(labels, header=None, names=['sheet', 'x1', 'y1', 'x2', 'y2', 'symbol'])\n    df['symbol'] = df['symbol'].apply(lambda x: x.replace('[\\'', ''))\n    df['symbol'] = df['symbol'].apply(lambda x: x.replace('\\']', ''))\n    classes = df.symbol.unique()\n    self.labels_dict = dict(zip(classes, [x for x in range(45)]))\n    self.labels = df\n    self.images = images\n    \n  def get_classname(self, value):\n    for classname, class_number in self.labels_dict.items():\n        if value == class_number:\n            return classname\n    return 'null'\n\n  def __len__(self):\n    return len(self.labels.sheet.unique())\n\n  def __getitem__(self, idx):\n    idx += 1\n    image = cv2.imread(self.images + '/' + str(idx) + '.png', 0)\n    image = image / 255\n    image_id = torch.tensor([idx])\n    label = self.labels[self.labels['sheet'] == idx]\n    \n    # Each label consists of fields: sheet number, x1, y1, x2, y2, label.\n    # Boxes shape: (n_objects, 4).\n    boxes = np.array(label[['x1', 'y1', 'x2', 'y2']]).reshape((-1, 4))\n    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n    \n    # Labels shape: (n_objects,).\n    labels = label[['symbol']]\n    labels = np.array([self.labels_dict.get(labels.iloc[i][0]) for i in range(len(labels))])\n    labels = torch.as_tensor(labels, dtype=torch.int64)\n    \n    target = { \"boxes\" : boxes, \"labels\" : labels, \"image_id\" : image_id }\n\n    return image, target","metadata":{"_uuid":"6716bd63-1af1-407a-8c02-88643d5a8ae3","_cell_guid":"88388ed2-bb56-43c2-ac8f-9aa12b14661a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-05-24T13:31:49.611669Z","iopub.execute_input":"2021-05-24T13:31:49.612285Z","iopub.status.idle":"2021-05-24T13:31:49.634436Z","shell.execute_reply.started":"2021-05-24T13:31:49.612242Z","shell.execute_reply":"2021-05-24T13:31:49.633623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, box_detections_per_img=500, box_score_thresh=0.4)\n\nnum_classes = 45\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"_uuid":"ec56cc78-2331-48e6-b7ab-e34a72728bc2","_cell_guid":"04b56d5c-73bb-43a1-b480-bd266acbf1cc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-05-24T13:31:49.638851Z","iopub.execute_input":"2021-05-24T13:31:49.639381Z","iopub.status.idle":"2021-05-24T13:31:50.59818Z","shell.execute_reply.started":"2021-05-24T13:31:49.639339Z","shell.execute_reply":"2021-05-24T13:31:50.597439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"\n    Describes how to combine tensors of different sizes (due to different number of objects on images).\n    \"\"\"\n    return tuple(zip(*batch))","metadata":{"_uuid":"7311da61-cef3-4e40-a04a-994575d389de","_cell_guid":"844582c6-c4ca-4c96-8af1-f6ea45f2cba5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-05-24T13:31:50.599385Z","iopub.execute_input":"2021-05-24T13:31:50.599737Z","iopub.status.idle":"2021-05-24T13:31:50.606283Z","shell.execute_reply.started":"2021-05-24T13:31:50.599702Z","shell.execute_reply":"2021-05-24T13:31:50.605322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = 3000\ntest_size = 1000\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndataset = BrailleDataset('../input/sheets-w-noise/sheets_w_noise', '../input/sheets-labels-w-noise/sheets_labels_w_noise.csv')\n\n# Shuffle indices.\nindices = torch.randperm(len(dataset)).tolist()\n# Slicing dataset into train and test.\ndataset_train = torch.utils.data.Subset(dataset, indices[:train_size])\ndataset_test = torch.utils.data.Subset(dataset, indices[-test_size:])\n\ntrain_loader = torch.utils.data.DataLoader(\n    dataset_train, batch_size=2, shuffle=True, collate_fn=collate_fn)\n\ntest_loader = torch.utils.data.DataLoader(\n    dataset_test, batch_size=2, shuffle=False, collate_fn=collate_fn)\n\nmodel.to(device)\n\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(params, lr=0.0001)","metadata":{"_uuid":"7ab68c28-7eaf-4f67-975b-7f4fbe7653f9","_cell_guid":"b4e92cb6-de6d-4fbd-826b-e4e66167f420","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-05-24T13:31:50.609719Z","iopub.execute_input":"2021-05-24T13:31:50.609972Z","iopub.status.idle":"2021-05-24T13:31:52.166083Z","shell.execute_reply.started":"2021-05-24T13:31:50.609947Z","shell.execute_reply":"2021-05-24T13:31:52.165113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset.labels_dict)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T13:31:52.168397Z","iopub.execute_input":"2021-05-24T13:31:52.168664Z","iopub.status.idle":"2021-05-24T13:31:52.175119Z","shell.execute_reply.started":"2021-05-24T13:31:52.168638Z","shell.execute_reply":"2021-05-24T13:31:52.173133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\ncolors = []\nfor i in range(num_classes):\n    r = random.randint(0, 255)\n    g = random.randint(0, 255)\n    b = random.randint(0, 255)\n    colors.append((r, g, b))\n\ncolors_classes = dict(zip([i for i in range(num_classes)], colors))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T13:31:52.176364Z","iopub.execute_input":"2021-05-24T13:31:52.176737Z","iopub.status.idle":"2021-05-24T13:31:52.185522Z","shell.execute_reply.started":"2021-05-24T13:31:52.176702Z","shell.execute_reply":"2021-05-24T13:31:52.184901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_boxes(boxes, labels, scores, image, epoch, image_id):\n    \"\"\"\n    Drawing boxes on the original image based on the network predictions. \n    \"\"\"\n    model.rpn_score_thresh = 0.8\n    image = cv2.cvtColor(image.astype('float32'), cv2.COLOR_GRAY2BGR)\n    for i, box in enumerate(boxes):\n        class_number = labels[i]\n        color = colors_classes.get(class_number)\n        cv2.rectangle(\n            image,\n            (int(box[0]), int(box[1])),\n            (int(box[2]), int(box[3])),\n            color, 2\n        )\n        cv2.putText(image, str(class_number) + ' ' + str(round(scores[i], 2)), (int(box[0]), int(box[1]-5)),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, \n                    lineType=cv2.LINE_AA)\n    ime = PIL.Image.fromarray((image * 255).astype(np.uint8))\n    ime.save(str(image_id.numpy()[0]) + '_Epoch:' + str(epoch) + '.png')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-24T13:31:52.188504Z","iopub.execute_input":"2021-05-24T13:31:52.189021Z","iopub.status.idle":"2021-05-24T13:31:52.198202Z","shell.execute_reply.started":"2021-05-24T13:31:52.188995Z","shell.execute_reply":"2021-05-24T13:31:52.197546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_iou(boxA, boxB):\n    \"\"\"\n    Calculating Intersection Over Union metric.\n    Args:\n        boxA (float array): ground truth box,\n        boxB (float array): predicted box.\n    \"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    inter_area = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n    \n    if inter_area == 0:\n        return 0\n    \n    boxA_area = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n    boxB_area = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n\n    iou = inter_area / float(boxA_area + boxB_area - inter_area)\n\n    return iou","metadata":{"execution":{"iopub.status.busy":"2021-05-24T13:31:52.199415Z","iopub.execute_input":"2021-05-24T13:31:52.19979Z","iopub.status.idle":"2021-05-24T13:31:52.212262Z","shell.execute_reply.started":"2021-05-24T13:31:52.199752Z","shell.execute_reply":"2021-05-24T13:31:52.211554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_metrics(gt_boxes, pred_boxes, target, labels):\n    \"\"\"\n    Calculating average Intersection Over Union, average classifiaction accuracy and correct detections percent. \n    \"\"\"\n    gts = gt_boxes.tolist()\n    preds = pred_boxes.tolist()\n    trgs = target[\"labels\"].numpy().tolist()\n    lbls = labels.tolist()\n    \n    correct_clf = 0.0\n    correct_det = 0.0\n    ious = []\n    \n    predictions = list(zip(preds, lbls))\n    targets = list(zip(gts, trgs))\n    \n    for (pred_box, l) in predictions:\n        \n        for (gt_box, t) in targets:\n            \n            iou = get_iou(gt_box, pred_box)\n            \n            if iou >= 0.5:\n                \n                correct_det += 1\n                \n                if l == t:\n                    correct_clf += 1\n                    \n                ious.append(iou)\n                targets.remove((gt_box, t))\n                \n    avg_iou = sum(ious) / len(ious)\n    clf_accuracy = correct_clf / len(preds)\n    correct_detections = correct_det / len(gts)\n    \n    return avg_iou, clf_accuracy, targets, correct_detections","metadata":{"execution":{"iopub.status.busy":"2021-05-24T13:31:52.214803Z","iopub.execute_input":"2021-05-24T13:31:52.215572Z","iopub.status.idle":"2021-05-24T13:31:52.223873Z","shell.execute_reply.started":"2021-05-24T13:31:52.215535Z","shell.execute_reply":"2021-05-24T13:31:52.223127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, epoch):\n    \"\"\"\n    Model evaluation for current epoch (drawing boxes, calculating metrics). \n    \"\"\"\n    i = 0\n    draw = True\n    ious = []\n    accuracies = []\n    undetected = []\n    detected = []\n    \n    for images, targets in test_loader:\n        \n        # Draw boxes only for first 3 samples.\n        if i == 3:\n            draw = False\n            \n        original_images = images    \n        images = list(torch.from_numpy(image).float().reshape((1, 2000, 1500)).to(device) for image in images)\n        outputs = model(images)\n        \n        original_images = list(original_images)\n        targets = list(targets)\n        \n        for image, output, target in zip(original_images, outputs, targets):\n            \n            gt_boxes = target[\"boxes\"].numpy()\n            pred_boxes = output[\"boxes\"].detach().cpu().numpy()\n            labels = output[\"labels\"].detach().cpu().numpy()\n            scores = output[\"scores\"].detach().cpu().numpy()\n        \n            if draw:\n                draw_boxes(pred_boxes, labels, scores, image, epoch, target[\"image_id\"])\n                \n            avg_iou, clf_accuracy, undetected_objects, det_obj_percent = get_metrics(gt_boxes, pred_boxes, target, labels)\n            \n            ious.append(avg_iou)\n            accuracies.append(clf_accuracy)\n            undetected.append(undetected_objects)\n            detected.append(det_obj_percent)\n        \n        i += 1\n    \n    # Calculating average metrics throughout the whole epoch.\n    average_iou = sum(ious) / len(ious)\n    average_accuracy = sum(accuracies) / len(accuracies)\n    predicted_boxes = sum(detected) / len(detected)\n        \n    return average_iou, average_accuracy, predicted_boxes, undetected","metadata":{"execution":{"iopub.status.busy":"2021-05-24T13:31:52.225251Z","iopub.execute_input":"2021-05-24T13:31:52.225693Z","iopub.status.idle":"2021-05-24T13:31:52.238782Z","shell.execute_reply.started":"2021-05-24T13:31:52.225655Z","shell.execute_reply":"2021-05-24T13:31:52.238038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 5\ndo_eval = True\n\nall_losses = []\nclf_losses = []\nbbox_losses = []\navg_ious = []\navg_accs = []\npred_boxes_num = []\nundetected_objs = []\n\nbest_boxes = 0.0","metadata":{"execution":{"iopub.status.busy":"2021-05-24T13:31:52.240958Z","iopub.execute_input":"2021-05-24T13:31:52.242016Z","iopub.status.idle":"2021-05-24T13:31:53.630182Z","shell.execute_reply.started":"2021-05-24T13:31:52.241977Z","shell.execute_reply":"2021-05-24T13:31:53.629144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training model.\n\nbest_model_wts = copy.deepcopy(model.state_dict())\n\nfor epoch in range(num_epochs):\n        \n    model.train()\n    \n    running_loss_cls = 0.0\n    running_loss_bbox = 0.0\n        \n    start = time.time()\n        \n    for images, targets in train_loader:\n            \n        images = list(torch.from_numpy(image).float().reshape((1, 2000, 1500)).to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n        loss_dict = model(images, targets)\n            \n        loss_classifier = loss_dict[\"loss_classifier\"]\n        loss_box_reg = loss_dict[\"loss_box_reg\"]      \n\n        losses = sum(loss for loss in loss_dict.values())\n\n        loss_value = losses.item()\n        running_loss_cls += loss_classifier\n        running_loss_bbox += loss_box_reg                                                        \n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n            \n    t = time.time() - start\n        \n    all_losses.append(loss_value)\n    clf_losses.append(running_loss_cls / train_size)\n    bbox_losses.append(running_loss_bbox / train_size)\n        \n    print('Epoch: {}, Loss_classifier: {:.4f}, Loss_box_reg: {:.4f}, Train time: {:.4f} min'.format(epoch, running_loss_cls / train_size, running_loss_bbox / train_size, t / 60))\n        \n    if do_eval:\n        model.eval()\n        average_iou, average_accuracy, predicted_boxes, undetected = evaluate(model, epoch)\n        avg_ious.append(average_iou)\n        avg_accs.append(average_accuracy)\n        pred_boxes_num.append(predicted_boxes)\n        undetected_objs.append(undetected)\n        print('Epoch: {}, Average IoU: {:.4f}, Average accuracy: {:.4f}, Average predicted boxes percent: {:.4f}'.format(epoch, average_iou, average_accuracy, predicted_boxes))\n\n        if best_boxes < predicted_boxes:\n            best_boxes = predicted_boxes\n            best_model_wts = copy.deepcopy(model.state_dict())","metadata":{"_uuid":"68762520-c705-4fdc-a6a5-ba1193f42ca6","_cell_guid":"856727fa-d56a-4186-9d82-55ff695d5973","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-05-24T13:31:53.631504Z","iopub.execute_input":"2021-05-24T13:31:53.631864Z","iopub.status.idle":"2021-05-24T14:36:21.959484Z","shell.execute_reply.started":"2021-05-24T13:31:53.631826Z","shell.execute_reply":"2021-05-24T14:36:21.958552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving best model weights.\n\nPATH = \"state_dict_model.pt\"\ntorch.save(best_model_wts, PATH)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T14:36:21.964027Z","iopub.execute_input":"2021-05-24T14:36:21.966206Z","iopub.status.idle":"2021-05-24T14:36:22.581204Z","shell.execute_reply.started":"2021-05-24T14:36:21.966162Z","shell.execute_reply":"2021-05-24T14:36:22.58013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(num_epochs), clf_losses)\nplt.plot(range(num_epochs), bbox_losses)\nplt.legend([\"Classification\", \"Bounding boxes\"])\nplt.title('Loss')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T14:36:22.583062Z","iopub.execute_input":"2021-05-24T14:36:22.583423Z","iopub.status.idle":"2021-05-24T14:36:22.739639Z","shell.execute_reply.started":"2021-05-24T14:36:22.583386Z","shell.execute_reply":"2021-05-24T14:36:22.738778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(num_epochs), avg_ious)\nplt.title('Intersection over Union')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T14:36:22.741189Z","iopub.execute_input":"2021-05-24T14:36:22.741564Z","iopub.status.idle":"2021-05-24T14:36:22.884729Z","shell.execute_reply.started":"2021-05-24T14:36:22.741527Z","shell.execute_reply":"2021-05-24T14:36:22.884021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(num_epochs), avg_accs)\nplt.title('Classification accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T14:36:22.887665Z","iopub.execute_input":"2021-05-24T14:36:22.88792Z","iopub.status.idle":"2021-05-24T14:36:23.031508Z","shell.execute_reply.started":"2021-05-24T14:36:22.887893Z","shell.execute_reply":"2021-05-24T14:36:23.029504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(num_epochs), pred_boxes_num)\nplt.title('Predicted boxes')","metadata":{"execution":{"iopub.status.busy":"2021-05-24T14:36:23.032987Z","iopub.execute_input":"2021-05-24T14:36:23.033326Z","iopub.status.idle":"2021-05-24T14:36:23.167318Z","shell.execute_reply.started":"2021-05-24T14:36:23.033287Z","shell.execute_reply":"2021-05-24T14:36:23.166562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting how well detection worked for all classes in comparison.\n\nimport collections\n\nflattened = [val for sublist in undetected_objs for val in sublist]\nflattened = [val for sublist in flattened for val in sublist]\n\nundetected_boxes, undetected_classes = map(list, zip(*flattened))\noccurrences = collections.Counter(undetected_classes)\nprint(occurrences)\n\noccs = dict(collections.OrderedDict(sorted(dict(occurrences).items())))\nx = occs.values()\nprint(x)\n\n#plt.bar([i for i in range(num_classes)], x, align='center', alpha=0.5, width=2)\n#plt.xticks([i for i in range(num_classes)])\n#plt.title('Detection percentage')\n\nplt.figure(figsize=(15, 5)) \nplt.bar(range(num_classes), x, align='edge', width=0.9)\nplt.xticks(range(num_classes))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T14:36:23.203554Z","iopub.execute_input":"2021-05-24T14:36:23.203816Z","iopub.status.idle":"2021-05-24T14:36:23.626297Z","shell.execute_reply.started":"2021-05-24T14:36:23.20379Z","shell.execute_reply":"2021-05-24T14:36:23.624019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numbers = { 'а': 1, 'б': 2, 'ц': 3, 'д': 4, 'е': 5, 'ф': 6, 'г': 7, 'х': 8, 'и': 9, 'ж': 0 }","metadata":{"execution":{"iopub.status.busy":"2021-05-24T14:36:23.627057Z","iopub.status.idle":"2021-05-24T14:36:23.627454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_to_text(pred_boxes, labels, n):\n    \"\"\"\n    Translating text in Braille to text in Russian based on network predictions.\n    Args:\n        pred_boxes (float array): predicted coordinates for letters,\n        labels (int array): predicted letters,\n        n (int): image serial number.\n    \"\"\"\n    text = \"\"\n    pred_boxes = pred_boxes.tolist()\n    labels = labels.tolist()\n    \n    fname = str(n) + '.txt'\n    f = open(fname, \"w\")\n    \n    is_number = False\n    is_capital = False\n    \n    # Networks returns predictions in random order so we need to\n    # sort boxes and group them (one group corresponds to one line of text).\n    \n    # Sorting predicted boxes by y1 (each box consists of [x1, y1, x2, y2]).\n    sorted_by_y = sorted(list(zip(pred_boxes, labels)), key = lambda x: x[0][1])\n    \n    lines = []\n    line = []\n    is_new_line = False\n    prev_box_y1 = sorted_by_y[0][0][1]\n    \n    for sym in sorted_by_y:\n        \n        curr_box_y1 = sym[0][1]\n        \n        # If distance between two symbols (vertically) is greater than approximate \n        # symbol height, this means we have a new line.\n        if abs(prev_box_y1 - curr_box_y1) > 50:\n            is_new_line = True\n            \n        prev_box_y1 = curr_box_y1\n        \n        if is_new_line and len(line) > 0:\n            lines.append(line)\n            line = []\n            is_new_line = False\n            \n        line.append(sym)\n        \n    # All symbols have been grouped into \"lines\".\n    lines.append(line)\n        \n    for l in lines:\n        \n        # Sorting symbols in each line by x1 to recreate letters order.\n        sorted_by_x = sorted(l, key = lambda x: x[0][0])\n        \n        prev_box_x1 = sorted_by_x[0][0][0]\n        \n        # Putting the text together.\n        for box, label in sorted_by_x:\n            \n            curr_box_x1 = box[0]\n            curr_box_x2 = box[2]\n        \n            symbol = dataset.get_classname(label)\n            \n            if symbol == 'цифровой символ':\n                is_number = True\n            \n            if symbol == 'знак заглавной буквы':\n                is_capital = True\n        \n            if abs(prev_box_x1 - curr_box_x1) > 80: #abs(curr_box_x1 - curr_box_x2) * 2:\n                if is_number == True:\n                    is_number = False\n                text = text + \" \"\n            \n            if is_capital and symbol != 'знак заглавной буквы':\n                symbol = symbol.upper()\n            elif is_number and symbol != 'цифровой символ':\n                symbol = str(numbers.get(symbol))\n            \n            is_capital = False\n        \n            if symbol != 'цифровой символ' and symbol != 'знак заглавной буквы':\n                text = text + symbol\n        \n            prev_box_x1 = curr_box_x1\n            \n        text += '\\n'\n    \n    f.write(text)\n    f.close()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T14:36:23.628647Z","iopub.status.idle":"2021-05-24T14:36:23.629175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Testing model on some real samples.\n\nn = 10000\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, box_detections_per_img=500, box_score_thresh=0.8)\nnum_classes = 45\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()\nmodel.to(device)\n\nfor root, dirs, files in os.walk('../input/real-examples/'):\n    files = [file[:-4] for file in files]\n    for filename in files:\n        filename = str(filename) + '.jpg'\n        image = cv2.imread('../input/real-examples/real_examples/' + filename, 0) / 255\n        image = cv2.resize(image, (1500, 2000))\n        original_image = image \n        images = [image]\n        images = list(torch.from_numpy(image).float().reshape((1, 2000, 1500)).to(device) for image in images)\n\n        output = model(images)[0]\n            \n        pred_boxes = output[\"boxes\"].detach().cpu().numpy()\n        labels = output[\"labels\"].detach().cpu().numpy()\n        scores = output[\"scores\"].detach().cpu().numpy()\n        \n        draw_boxes(pred_boxes, labels, scores, image, n, torch.tensor([n]))\n        \n        image_to_text(pred_boxes, labels, n)\n        \n        n += 1","metadata":{"execution":{"iopub.status.busy":"2021-05-24T14:36:23.630144Z","iopub.status.idle":"2021-05-24T14:36:23.630728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}